# Organoid Segmentation Pipeline

This repository contains the pipeline for processing Stereo-seq organoid data, from raw GEF files to segmented Parquet datasets. It is designed to work side-by-side with the [Segger](https://github.com/nrclaudio/stereosegger) repository.

## Directory Structure

```
segmentation/              (Project Root)
├── segger/                (The Segger library code)
└── organoid_segmentation_pipeline/  (This pipeline code)
    ├── slurm_export_h5ad.sh   (HPC script for GEF -> H5AD)
    ├── sto_export_h5ad.py     (Conversion script used by SLURM)
    ├── process_all_chips.py   (Pipeline Step 1)
    ├── prepare_segger_inputs.py (Pipeline Step 2)
    ├── run_segger_pipeline.py (Pipeline Step 3)
    └── ...
```

## Prerequisites & Input Data

The pipeline expects a specific directory structure for the input data, typically generated by the upstream alignment pipeline.

**Expected Structure:**
```
realigned_{CHIP_ID}/
└── ... /outs/feature_expression/
    ├── {CHIP_ID}.tissue.gef      <-- Raw Input (Linux/HPC only)
    └── {CHIP_ID}_tissue.h5ad     <-- Converted Input (Mac/Local)
```

## Workflow

### Step 0: Pre-processing (Linux / HPC)
*Requires `stereopy` (Linux only).*

Before running the main pipeline on macOS, you must convert the raw `.tissue.gef` files to `.h5ad`. This is typically done on an HPC cluster.

1.  Edit `slurm_export_h5ad.sh` to match your HPC paths.
2.  Submit the job:
    ```bash
    sbatch slurm_export_h5ad.sh
    ```
    This will generate `{CHIP_ID}_tissue.h5ad` next to each `.gef` file.

---

### Step 1: Local / Workstation Processing (macOS or Linux)
*Requires `segger` environment (see `../segger/README.md`).*

Once you have the `_tissue.h5ad` files locally or on your workstation (mirroring the folder structure), you can run the segmentation pipeline.

**1. Extract Kidneys:**
Converts H5AD + Images -> SpatialData -> Individual Kidney Crops.
```bash
python process_all_chips.py
```
*Output: `kidneys/` folder containing `.sdata.zarr` files.*

**2. Prepare Segger Inputs:**
Converts Kidney Zarrs -> Segger Parquet Datasets (with graph structures).
```bash
python prepare_segger_inputs.py
```
*Output: `segger_inputs/` folder.*

**3. Run Segmentation/Training:**
Runs the Segger GNN pipeline.
```bash
python run_segger_pipeline.py
```
*Output: `segger_datasets/` and `segger_models/`.*

### Option: Parallel Execution with Snakemake

For faster processing (especially for Step 2 and 3), you can use the included `Snakefile`.

1.  **Run Step 1** manually to extract kidneys:
    ```bash
    python process_all_chips.py
    ```
2.  **Run Snakemake** to parallelize preparation and training:
    ```bash
    # Install snakemake if needed: pip install snakemake
    snakemake --cores 4
    ```

### Docker Usage

To build the Docker image, you must run the build command **from the project root** (`segmentation/`) so that it can access both `segger` and the pipeline.

**Build:**
```bash
# From 'segmentation/' root:
docker build -t organoid-pipeline -f organoid_segmentation_pipeline/Dockerfile .
```

**Run:**
```bash
docker run -it -v $(pwd):/data organoid-pipeline
```
